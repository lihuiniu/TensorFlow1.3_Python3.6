{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "step 0, training accuracy 0.1\n",
      "step 100, training accuracy 0.88\n",
      "step 200, training accuracy 0.88\n",
      "step 300, training accuracy 0.94\n",
      "step 400, training accuracy 0.96\n",
      "step 500, training accuracy 0.92\n",
      "step 600, training accuracy 0.88\n",
      "step 700, training accuracy 0.92\n",
      "step 800, training accuracy 0.96\n",
      "step 900, training accuracy 0.98\n",
      "step 1000, training accuracy 1\n",
      "step 1100, training accuracy 0.96\n",
      "step 1200, training accuracy 0.98\n",
      "step 1300, training accuracy 0.98\n",
      "step 1400, training accuracy 0.98\n",
      "step 1500, training accuracy 0.96\n",
      "step 1600, training accuracy 0.98\n",
      "step 1700, training accuracy 1\n",
      "step 1800, training accuracy 0.96\n",
      "step 1900, training accuracy 0.98\n",
      "step 2000, training accuracy 0.94\n",
      "step 2100, training accuracy 0.98\n",
      "step 2200, training accuracy 0.98\n",
      "step 2300, training accuracy 0.96\n",
      "step 2400, training accuracy 0.98\n",
      "step 2500, training accuracy 0.96\n",
      "step 2600, training accuracy 0.98\n",
      "step 2700, training accuracy 0.98\n",
      "step 2800, training accuracy 1\n",
      "step 2900, training accuracy 0.92\n",
      "step 3000, training accuracy 0.92\n",
      "step 3100, training accuracy 0.92\n",
      "step 3200, training accuracy 1\n",
      "step 3300, training accuracy 0.98\n",
      "step 3400, training accuracy 1\n",
      "step 3500, training accuracy 1\n",
      "step 3600, training accuracy 1\n",
      "step 3700, training accuracy 1\n",
      "step 3800, training accuracy 1\n",
      "step 3900, training accuracy 0.92\n",
      "step 4000, training accuracy 0.98\n",
      "step 4100, training accuracy 0.96\n",
      "step 4200, training accuracy 0.98\n",
      "step 4300, training accuracy 0.98\n",
      "step 4400, training accuracy 1\n",
      "step 4500, training accuracy 1\n",
      "step 4600, training accuracy 0.98\n",
      "step 4700, training accuracy 0.98\n",
      "step 4800, training accuracy 0.98\n",
      "step 4900, training accuracy 0.98\n",
      "step 5000, training accuracy 0.98\n",
      "step 5100, training accuracy 1\n",
      "step 5200, training accuracy 1\n",
      "step 5300, training accuracy 1\n",
      "step 5400, training accuracy 1\n",
      "step 5500, training accuracy 0.98\n",
      "step 5600, training accuracy 0.98\n",
      "step 5700, training accuracy 1\n",
      "step 5800, training accuracy 1\n",
      "step 5900, training accuracy 1\n",
      "step 6000, training accuracy 0.96\n",
      "step 6100, training accuracy 1\n",
      "step 6200, training accuracy 0.98\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "\n",
    "def weight_variable_whale(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "def bias_variable_whale(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "def conv2d_whale(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "def max_pool_2x2_whale(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                         strides=[1, 2, 2, 1], padding='SAME')\n",
    "#The first convolution will compute 32 features for each 5*5 patch.\n",
    "#1 is the number of input channels.\n",
    "#32 is the number of output channels.\n",
    "W_conv1 = weight_variable_whale([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable_whale([32])\n",
    "#with tf.name_scope('input'):\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='x-input')\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='y-input')\n",
    "y_ = tf.placeholder(tf.float32, [None, 10], name='y_-input')\n",
    "#with tf.name_scope('input_reshape'):\n",
    "image_shaped_input = tf.reshape(x, [-1, 28, 28, 1])\n",
    "tf.summary.image('input', image_shaped_input, 10)\n",
    "#reshape x to 4d tensor, with the second and third parameters corresponding to image width and height.\n",
    "# and the final dimension corresponding to the number of color channels.\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d_whale(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2_whale(h_conv1) #reduce the image size to 14*14\n",
    "\n",
    "#The second layer will have 64 features for each 5*5 patch.\n",
    "W_conv2 = weight_variable_whale([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable_whale([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d_whale(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2_whale(h_conv2)\n",
    "\n",
    "#Now that the image size has been reduced to 7*7.\n",
    "#We add a fully-connected layer witg 1024 neurons to allow processing on the entire image.\n",
    "#We reshape the tensor from the pooling layer into a batch of vectors\n",
    "W_fc1 = weight_variable_whale([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable_whale([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable_whale([1024, 10])\n",
    "b_fc2 = bias_variable_whale([10])\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(20000):\n",
    "        batch = mnist.train.next_batch(50)\n",
    "        if i % 100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={\n",
    "                x: batch[0], y_: batch[1], keep_prob: 1.0\n",
    "            })\n",
    "            print('step %d, training accuracy %g' %(i, train_accuracy))\n",
    "        train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "    print('test accuracy %g' %accuracy.eval(feed_dict={\n",
    "        x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0\n",
    "    }))\n",
    "    \n",
    "    \n",
    "def variable_summaries(var):\n",
    "  \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "  with tf.name_scope('summaries'):\n",
    "    mean = tf.reduce_mean(var)\n",
    "    tf.summary.scalar('mean', mean)\n",
    "    with tf.name_scope('stddev'):\n",
    "      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "    tf.summary.scalar('stddev', stddev)\n",
    "    tf.summary.scalar('max', tf.reduce_max(var))\n",
    "    tf.summary.scalar('min', tf.reduce_min(var))\n",
    "    tf.summary.histogram('histogram', var)\n",
    "\n",
    "def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n",
    "  \"\"\"Reusable code for making a simple neural net layer.\n",
    "\n",
    "  It does a matrix multiply, bias add, and then uses relu to nonlinearize.\n",
    "  It also sets up name scoping so that the resultant graph is easy to read,\n",
    "  and adds a number of summary ops.\n",
    "  \"\"\"\n",
    "  # Adding a name scope ensures logical grouping of the layers in the graph.\n",
    "  with tf.name_scope(layer_name):\n",
    "    # This Variable will hold the state of the weights for the layer\n",
    "    with tf.name_scope('weights'):\n",
    "      weights = weight_variable([input_dim, output_dim])\n",
    "      variable_summaries(weights)\n",
    "    with tf.name_scope('biases'):\n",
    "      biases = bias_variable([output_dim])\n",
    "      variable_summaries(biases)\n",
    "    with tf.name_scope('Wx_plus_b'):\n",
    "      preactivate = tf.matmul(input_tensor, weights) + biases\n",
    "      tf.summary.histogram('pre_activations', preactivate)\n",
    "    activations = act(preactivate, name='activation')\n",
    "    tf.summary.histogram('activations', activations)\n",
    "    return activations\n",
    "\n",
    "hidden1 = nn_layer(x, 784, 500, 'layer1')\n",
    "\n",
    "with tf.name_scope('dropout'):\n",
    "  keep_prob = tf.placeholder(tf.float32)\n",
    "  tf.summary.scalar('dropout_keep_probability', keep_prob)\n",
    "  dropped = tf.nn.dropout(hidden1, keep_prob)\n",
    "\n",
    "# Do not apply softmax activation yet, see below.\n",
    "y = nn_layer(dropped, 500, 10, 'layer2', act=tf.identity)\n",
    "\n",
    "with tf.name_scope('cross_entropy'):\n",
    "  # The raw formulation of cross-entropy,\n",
    "  #\n",
    "  # tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.softmax(y)),\n",
    "  #                               reduction_indices=[1]))\n",
    "  #\n",
    "  # can be numerically unstable.\n",
    "  #\n",
    "  # So here we use tf.nn.softmax_cross_entropy_with_logits on the\n",
    "  # raw outputs of the nn_layer above, and then average across\n",
    "  # the batch.\n",
    "  diff = tf.nn.softmax_cross_entropy_with_logits(targets=y_, logits=y)\n",
    "  with tf.name_scope('total'):\n",
    "    cross_entropy = tf.reduce_mean(diff)\n",
    "tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "  train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(\n",
    "      cross_entropy)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "  with tf.name_scope('correct_prediction'):\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "  with tf.name_scope('accuracy'):\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "# Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(FLAGS.summaries_dir + '/train',\n",
    "                                      sess.graph)\n",
    "test_writer = tf.summary.FileWriter(FLAGS.summaries_dir + '/test')\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "\n",
    "\n",
    "# Train the model, and also write summaries.\n",
    "# Every 10th step, measure test-set accuracy, and write test summaries\n",
    "# All other steps, run train_step on training data, & add training summaries\n",
    "\n",
    "def feed_dict(train):\n",
    "  \"\"\"Make a TensorFlow feed_dict: maps data onto Tensor placeholders.\"\"\"\n",
    "  if train or FLAGS.fake_data:\n",
    "    xs, ys = mnist.train.next_batch(100, fake_data=FLAGS.fake_data)\n",
    "    k = FLAGS.dropout\n",
    "  else:\n",
    "    xs, ys = mnist.test.images, mnist.test.labels\n",
    "    k = 1.0\n",
    "  return {x: xs, y_: ys, keep_prob: k}\n",
    "\n",
    "for i in range(FLAGS.max_steps):\n",
    "  if i % 10 == 0:  # Record summaries and test-set accuracy\n",
    "    summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))\n",
    "    test_writer.add_summary(summary, i)\n",
    "    print('Accuracy at step %s: %s' % (i, acc))\n",
    "  else:  # Record train set summaries, and train\n",
    "    summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))\n",
    "    train_writer.add_summary(summary, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
